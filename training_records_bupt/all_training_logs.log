2025-12-03 16:59:30 - 
==================== New Experiment Started: 2025-12-03_16-59-30 ====================
2025-12-03 16:59:30 - Run Directory: training_records_bupt/2025-12-03_16-59-30
2025-12-03 16:59:30 - Arguments: Namespace(dataset='BUPT', lr=0.005, hid=64, dropout=0.2, adj_dropout=0.1, attn_drop=0.1, in_drop=0.1, attention_weight=0.5, feature_weight=0.5, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=400, patience=50, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.01, print_interval=50, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-03 16:59:30 - Initializing weights using Static HAS-GNN Priors...
2025-12-03 16:59:30 - |This is the 1th layer!|
2025-12-03 16:59:34 - [Layer 1] Ep 0: Loss=1.1195/1.0489 | AUC=0.5132/0.7690
2025-12-03 16:59:43 - [Layer 1] Ep 50: Loss=0.5131/0.4394 | AUC=0.9145/0.9189
2025-12-03 16:59:53 - [Layer 1] Ep 100: Loss=0.4751/0.3914 | AUC=0.9236/0.9284
2025-12-03 17:00:02 - [Layer 1] Ep 150: Loss=0.4512/0.3646 | AUC=0.9295/0.9330
2025-12-03 17:00:11 - [Layer 1] Ep 200: Loss=0.4394/0.3499 | AUC=0.9319/0.9364
2025-12-03 17:00:21 - [Layer 1] Ep 250: Loss=0.4303/0.3370 | AUC=0.9341/0.9378
2025-12-03 17:00:30 - [Layer 1] Ep 300: Loss=0.4298/0.3322 | AUC=0.9340/0.9389
2025-12-03 17:00:40 - [Layer 1] Ep 350: Loss=0.4288/0.3288 | AUC=0.9343/0.9390
2025-12-03 17:00:49 - |This is the 2th layer!|
2025-12-03 17:00:49 - [Layer 2] Ep 0: Loss=1.2119/0.9004 | AUC=0.8401/0.8424
2025-12-03 17:00:57 - [Layer 2] Ep 50: Loss=0.9192/1.1590 | AUC=0.8596/0.8719
2025-12-03 17:01:05 - [Layer 2] Ep 100: Loss=0.8927/1.1299 | AUC=0.8617/0.8731
2025-12-03 17:01:12 - [Layer 2] Ep 150: Loss=0.8714/1.1121 | AUC=0.8609/0.8710
2025-12-03 17:01:20 - [Layer 2] Ep 200: Loss=0.8680/1.1023 | AUC=0.8586/0.8713
2025-12-03 17:01:28 - [Layer 2] Ep 250: Loss=0.8623/1.1005 | AUC=0.8593/0.8692
2025-12-03 17:01:36 - [Layer 2] Ep 300: Loss=0.8618/1.0956 | AUC=0.8586/0.8691
2025-12-03 17:01:44 - [Layer 2] Ep 350: Loss=0.8554/1.1006 | AUC=0.8597/0.8699
2025-12-03 17:01:51 - 
Final Test Report:
              precision    recall  f1-score   support

      Normal     0.9773    0.8483    0.9083     19973
       Fraud     0.6714    0.9242    0.7778      1689
     Courier     0.3331    0.7455    0.4604      1615

    accuracy                         0.8467     23277
   macro avg     0.6606    0.8394    0.7155     23277
weighted avg     0.9104    0.8467    0.8677     23277

2025-12-03 17:01:51 - Final Result -> Macro AUC: 0.9416, Macro F1: 0.7155, Macro Recall: 0.8394, G-mean: 0.8747, Acc: 0.8467
2025-12-03 17:09:21 - 
==================== New Experiment Started: 2025-12-03_17-09-21 ====================
2025-12-03 17:09:21 - Run Directory: training_records_bupt/2025-12-03_17-09-21
2025-12-03 17:09:21 - Arguments: Namespace(dataset='BUPT', lr=0.005, hid=64, dropout=0.2, adj_dropout=0.1, attn_drop=0.1, in_drop=0.1, attention_weight=0.5, feature_weight=0.5, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=400, patience=50, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.01, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=0.5, lambda_G=0.5, lambda_D=0.5)
2025-12-03 17:09:21 - Initializing weights using Static HAS-GNN Priors...
2025-12-03 17:09:21 - |This is the 1th layer!|
2025-12-03 17:09:24 - [Layer 1] Ep 0: Loss=1.1192/1.0471 | AUC=0.5132/0.7679
2025-12-03 17:09:33 - [Layer 1] Ep 50: Loss=0.5142/0.4358 | AUC=0.9146/0.9192
2025-12-03 17:09:42 - [Layer 1] Ep 100: Loss=0.4766/0.3892 | AUC=0.9235/0.9286
2025-12-03 17:09:51 - [Layer 1] Ep 150: Loss=0.4527/0.3621 | AUC=0.9295/0.9330
2025-12-03 17:10:01 - [Layer 1] Ep 200: Loss=0.4406/0.3475 | AUC=0.9320/0.9364
2025-12-03 17:10:10 - [Layer 1] Ep 250: Loss=0.4318/0.3348 | AUC=0.9340/0.9378
2025-12-03 17:10:20 - [Layer 1] Ep 300: Loss=0.4314/0.3310 | AUC=0.9340/0.9389
2025-12-03 17:10:29 - [Layer 1] Ep 350: Loss=0.4299/0.3272 | AUC=0.9344/0.9389
2025-12-03 17:10:38 - |This is the 2th layer!|
2025-12-03 17:10:39 - [Layer 2] Ep 0: Loss=1.2468/0.9004 | AUC=0.8405/0.8402
2025-12-03 17:10:46 - [Layer 2] Ep 50: Loss=0.8498/1.5871 | AUC=0.7999/0.8176
2025-12-03 17:10:54 - [Layer 2] Ep 100: Loss=0.8081/1.6057 | AUC=0.7859/0.8069
2025-12-03 17:11:01 - [Layer 2] Ep 150: Loss=0.7870/1.6044 | AUC=0.7817/0.8035
2025-12-03 17:11:09 - [Layer 2] Ep 200: Loss=0.7868/1.6038 | AUC=0.7793/0.8020
2025-12-03 17:11:17 - [Layer 2] Ep 250: Loss=0.7761/1.6013 | AUC=0.7784/0.7999
2025-12-03 17:12:09 - [Layer 2] Ep 300: Loss=0.7665/1.5992 | AUC=0.7798/0.7990
2025-12-03 17:12:16 - [Layer 2] Ep 350: Loss=0.7592/1.5981 | AUC=0.7761/0.7977
2025-12-03 17:12:24 - 
Final Test Report:
              precision    recall  f1-score   support

      Normal     0.9803    0.7440    0.8459     19973
       Fraud     0.6177    0.9278    0.7416      1689
     Courier     0.2273    0.7858    0.3526      1615

    accuracy                         0.7602     23277
   macro avg     0.6084    0.8192    0.6467     23277
weighted avg     0.9018    0.7602    0.8041     23277

2025-12-03 17:12:24 - Final Result -> Macro AUC: 0.9369, Macro F1: 0.6467, Macro Recall: 0.8192, G-mean: 0.8532, Acc: 0.7602
2025-12-03 17:15:12 - 
==================== New Experiment Started: 2025-12-03_17-15-12 ====================
2025-12-03 17:15:12 - Run Directory: training_records_bupt/2025-12-03_17-15-12
2025-12-03 17:15:12 - Arguments: Namespace(dataset='BUPT', lr=0.005, hid=64, dropout=0.2, adj_dropout=0.1, attn_drop=0.1, in_drop=0.1, attention_weight=0.5, feature_weight=0.5, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=400, patience=50, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.01, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-03 17:15:12 - Initializing weights using Static HAS-GNN Priors...
2025-12-03 17:15:12 - |This is the 1th layer!|
2025-12-03 17:15:16 - [Layer 1] Ep 0: Loss=1.1195/1.0489 | AUC=0.5132/0.7690
2025-12-03 17:15:24 - [Layer 1] Ep 50: Loss=0.5131/0.4394 | AUC=0.9145/0.9189
2025-12-03 17:15:34 - [Layer 1] Ep 100: Loss=0.4751/0.3914 | AUC=0.9236/0.9284
2025-12-03 17:15:43 - [Layer 1] Ep 150: Loss=0.4512/0.3646 | AUC=0.9295/0.9330
2025-12-03 17:15:52 - [Layer 1] Ep 200: Loss=0.4394/0.3499 | AUC=0.9319/0.9364
2025-12-03 17:16:02 - [Layer 1] Ep 250: Loss=0.4303/0.3370 | AUC=0.9341/0.9378
2025-12-03 17:16:11 - [Layer 1] Ep 300: Loss=0.4298/0.3322 | AUC=0.9340/0.9389
2025-12-03 17:16:20 - [Layer 1] Ep 350: Loss=0.4289/0.3288 | AUC=0.9343/0.9390
2025-12-03 17:16:29 - |This is the 2th layer!|
2025-12-03 17:16:29 - [Layer 2] Ep 0: Loss=1.3233/0.9020 | AUC=0.8404/0.8397
2025-12-03 17:16:37 - [Layer 2] Ep 50: Loss=0.8573/1.6680 | AUC=0.7638/0.7843
2025-12-03 17:16:44 - [Layer 2] Ep 100: Loss=0.8173/1.6879 | AUC=0.7459/0.7686
2025-12-03 17:16:52 - [Layer 2] Ep 150: Loss=0.7940/1.6845 | AUC=0.7417/0.7633
2025-12-03 17:17:00 - [Layer 2] Ep 200: Loss=0.7801/1.6873 | AUC=0.7469/0.7669
2025-12-03 17:17:07 - [Layer 2] Ep 250: Loss=0.7812/1.6856 | AUC=0.7414/0.7596
2025-12-03 17:17:14 - [Layer 2] Ep 300: Loss=0.7750/1.7020 | AUC=0.7433/0.7595
2025-12-03 17:18:12 - [Layer 2] Ep 350: Loss=0.7687/1.6932 | AUC=0.7409/0.7587
2025-12-03 17:19:11 - 
Final Test Report:
              precision    recall  f1-score   support

      Normal     0.9802    0.7286    0.8359     19973
       Fraud     0.5823    0.9402    0.7192      1689
     Courier     0.2190    0.7734    0.3413      1615

    accuracy                         0.7470     23277
   macro avg     0.5938    0.8141    0.6321     23277
weighted avg     0.8985    0.7470    0.7931     23277

2025-12-03 17:19:11 - Final Result -> Macro AUC: 0.9359, Macro F1: 0.6321, Macro Recall: 0.8141, G-mean: 0.8484, Acc: 0.7470
2025-12-03 17:22:39 - 
==================== New Experiment Started: 2025-12-03_17-22-39 ====================
2025-12-03 17:22:39 - Run Directory: training_records_bupt/2025-12-03_17-22-39
2025-12-03 17:22:39 - Arguments: Namespace(dataset='BUPT', lr=0.01, hid=64, dropout=0.2, adj_dropout=0.1, attn_drop=0.1, in_drop=0.1, attention_weight=0.5, feature_weight=0.6, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=400, patience=50, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.01, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-03 17:22:39 - Initializing weights using Static HAS-GNN Priors...
2025-12-03 17:22:39 - |This is the 1th layer!|
2025-12-03 17:22:43 - [Layer 1] Ep 0: Loss=1.1195/1.0224 | AUC=0.5132/0.8346
2025-12-03 17:22:52 - [Layer 1] Ep 50: Loss=0.4830/0.4024 | AUC=0.9215/0.9260
2025-12-03 17:23:01 - [Layer 1] Ep 100: Loss=0.4493/0.3622 | AUC=0.9296/0.9341
2025-12-03 17:23:10 - [Layer 1] Ep 150: Loss=0.4316/0.3365 | AUC=0.9341/0.9381
2025-12-03 17:23:20 - [Layer 1] Ep 200: Loss=0.4280/0.3291 | AUC=0.9346/0.9391
2025-12-03 17:23:29 - [Layer 1] Ep 250: Loss=0.4235/0.3254 | AUC=0.9356/0.9391
2025-12-03 17:23:40 - [Layer 1] Ep 300: Loss=0.4234/0.3196 | AUC=0.9354/0.9393
2025-12-03 17:23:49 - [Layer 1] Ep 350: Loss=0.4231/0.3200 | AUC=0.9353/0.9398
2025-12-03 17:23:58 - |This is the 2th layer!|
2025-12-03 17:23:59 - [Layer 2] Ep 0: Loss=1.2806/0.8961 | AUC=0.8424/0.8378
2025-12-03 17:24:06 - [Layer 2] Ep 50: Loss=0.8047/1.6760 | AUC=0.7365/0.7590
2025-12-03 17:24:15 - [Layer 2] Ep 100: Loss=0.7737/1.6808 | AUC=0.7475/0.7647
2025-12-03 17:24:23 - [Layer 2] Ep 150: Loss=0.7682/1.6782 | AUC=0.7434/0.7647
2025-12-03 17:24:31 - [Layer 2] Ep 200: Loss=0.7655/1.6740 | AUC=0.7463/0.7658
2025-12-03 17:24:38 - [Layer 2] Ep 250: Loss=0.7660/1.6715 | AUC=0.7460/0.7602
2025-12-03 17:24:46 - [Layer 2] Ep 300: Loss=0.7518/1.6712 | AUC=0.7437/0.7633
2025-12-03 17:24:54 - [Layer 2] Ep 350: Loss=0.7539/1.6720 | AUC=0.7459/0.7629
2025-12-03 17:25:01 - 
Final Test Report:
              precision    recall  f1-score   support

      Normal     0.9800    0.7420    0.8445     19973
       Fraud     0.5973    0.9414    0.7309      1689
     Courier     0.2272    0.7728    0.3512      1615

    accuracy                         0.7586     23277
   macro avg     0.6015    0.8187    0.6422     23277
weighted avg     0.9000    0.7586    0.8021     23277

2025-12-03 17:25:01 - Final Result -> Macro AUC: 0.9361, Macro F1: 0.6422, Macro Recall: 0.8187, G-mean: 0.8524, Acc: 0.7586
