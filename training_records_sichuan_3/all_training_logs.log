2025-12-02 10:00:57 - 
==================== New Experiment Started: 2025-12-02_10-00-57 ====================
2025-12-02 10:00:57 - Run Directory: training_records_sichuan_3/2025-12-02_10-00-57
2025-12-02 10:00:57 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.002, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=400, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:00:58 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:00:58 - |This is the 1th layer!|
2025-12-02 10:01:00 - [Layer 1] Ep 0: Loss=1.3680/0.6743 | AUC=0.5305/0.6998
2025-12-02 10:01:02 - [Layer 1] Ep 50: Loss=0.5385/0.2953 | AUC=0.9481/0.9347
2025-12-02 10:01:04 - [Layer 1] Ep 100: Loss=0.4814/0.2706 | AUC=0.9652/0.9445
2025-12-02 10:01:06 - [Layer 1] Ep 150: Loss=0.4609/0.2662 | AUC=0.9693/0.9458
2025-12-02 10:01:08 - [Layer 1] Ep 200: Loss=0.4425/0.2679 | AUC=0.9749/0.9463
2025-12-02 10:01:10 - [Layer 1] Ep 250: Loss=0.4217/0.2694 | AUC=0.9768/0.9459
2025-12-02 10:01:12 - [Layer 1] Ep 300: Loss=0.4128/0.2731 | AUC=0.9808/0.9454
2025-12-02 10:01:14 - [Layer 1] Ep 350: Loss=0.4083/0.2750 | AUC=0.9817/0.9453
2025-12-02 10:01:15 - |This is the 2th layer!|
2025-12-02 10:01:16 - [Layer 2] Ep 0: Loss=1.1352/0.6800 | AUC=0.9176/0.9110
2025-12-02 10:01:18 - [Layer 2] Ep 50: Loss=1.0152/0.6894 | AUC=0.7241/0.7301
2025-12-02 10:01:20 - [Layer 2] Ep 100: Loss=0.8351/0.7296 | AUC=0.6121/0.6336
2025-12-02 10:01:21 - [Layer 2] Ep 150: Loss=0.7582/0.7742 | AUC=0.5815/0.6105
2025-12-02 10:01:23 - [Layer 2] Ep 200: Loss=0.7083/0.7955 | AUC=0.5756/0.6055
2025-12-02 10:01:25 - [Layer 2] Ep 250: Loss=0.6367/0.8067 | AUC=0.5764/0.6071
2025-12-02 10:01:27 - [Layer 2] Ep 300: Loss=0.6614/0.8120 | AUC=0.5808/0.6082
2025-12-02 10:01:29 - [Layer 2] Ep 350: Loss=0.6057/0.8166 | AUC=0.5842/0.6076
2025-12-02 10:01:31 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9403    0.9290    0.9346       831
           1     0.8529    0.8747    0.8636       391

    accuracy                         0.9116      1222
   macro avg     0.8966    0.9018    0.8991      1222
weighted avg     0.9123    0.9116    0.9119      1222

2025-12-02 10:01:31 - Final Result -> Macro AUC: 0.9452, Macro F1: 0.8991, Macro Recall: 0.9018, G-mean: 0.9014, Acc: 0.9116
2025-12-02 10:18:56 - 
==================== New Experiment Started: 2025-12-02_10-18-56 ====================
2025-12-02 10:18:56 - Run Directory: training_records_sichuan_3/2025-12-02_10-18-56
2025-12-02 10:18:56 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.002, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=400, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:18:56 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:18:56 - |This is the 1th layer!|
2025-12-02 10:19:00 - [Layer 1] Ep 0: Loss=1.3680/0.6743 | AUC=0.5305/0.6998
2025-12-02 10:19:02 - [Layer 1] Ep 50: Loss=0.5385/0.2953 | AUC=0.9481/0.9347
2025-12-02 10:19:03 - [Layer 1] Ep 100: Loss=0.4814/0.2706 | AUC=0.9652/0.9445
2025-12-02 10:19:05 - [Layer 1] Ep 150: Loss=0.4609/0.2662 | AUC=0.9693/0.9458
2025-12-02 10:19:08 - [Layer 1] Ep 200: Loss=0.4425/0.2679 | AUC=0.9749/0.9463
2025-12-02 10:19:09 - [Layer 1] Ep 250: Loss=0.4217/0.2694 | AUC=0.9768/0.9459
2025-12-02 10:19:11 - [Layer 1] Ep 300: Loss=0.4129/0.2731 | AUC=0.9808/0.9454
2025-12-02 10:19:14 - [Layer 1] Ep 350: Loss=0.4084/0.2750 | AUC=0.9817/0.9453
2025-12-02 10:19:15 - |This is the 2th layer!|
2025-12-02 10:19:16 - [Layer 2] Ep 0: Loss=1.0339/0.6792 | AUC=0.9171/0.9110
2025-12-02 10:19:18 - [Layer 2] Ep 50: Loss=0.9482/0.6464 | AUC=0.8836/0.8897
2025-12-02 10:19:20 - [Layer 2] Ep 100: Loss=0.8862/0.6145 | AUC=0.8822/0.8892
2025-12-02 10:19:21 - [Layer 2] Ep 150: Loss=0.8620/0.5920 | AUC=0.8835/0.8903
2025-12-02 10:19:24 - [Layer 2] Ep 200: Loss=0.8461/0.5783 | AUC=0.8854/0.8911
2025-12-02 10:19:25 - [Layer 2] Ep 250: Loss=0.8359/0.5706 | AUC=0.8860/0.8916
2025-12-02 10:19:27 - [Layer 2] Ep 300: Loss=0.8305/0.5665 | AUC=0.8866/0.8918
2025-12-02 10:19:29 - [Layer 2] Ep 350: Loss=0.8284/0.5648 | AUC=0.8878/0.8926
2025-12-02 10:19:31 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9374    0.9555    0.9464       831
           1     0.9013    0.8645    0.8825       391

    accuracy                         0.9264      1222
   macro avg     0.9194    0.9100    0.9144      1222
weighted avg     0.9259    0.9264    0.9259      1222

2025-12-02 10:19:31 - Final Result -> Macro AUC: 0.9504, Macro F1: 0.9144, Macro Recall: 0.9100, G-mean: 0.9088, Acc: 0.9264
2025-12-02 10:30:22 - 
==================== New Experiment Started: 2025-12-02_10-30-22 ====================
2025-12-02 10:30:22 - Run Directory: training_records_sichuan_3/2025-12-02_10-30-22
2025-12-02 10:30:22 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.002, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=600, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:30:22 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:30:22 - |This is the 1th layer!|
2025-12-02 10:30:24 - [Layer 1] Ep 0: Loss=1.3680/0.6743 | AUC=0.5305/0.6998
2025-12-02 10:30:26 - [Layer 1] Ep 50: Loss=0.5385/0.2953 | AUC=0.9481/0.9347
2025-12-02 10:30:28 - [Layer 1] Ep 100: Loss=0.4814/0.2706 | AUC=0.9652/0.9445
2025-12-02 10:30:30 - [Layer 1] Ep 150: Loss=0.4609/0.2662 | AUC=0.9693/0.9458
2025-12-02 10:30:31 - [Layer 1] Ep 200: Loss=0.4425/0.2679 | AUC=0.9749/0.9463
2025-12-02 10:30:33 - [Layer 1] Ep 250: Loss=0.4218/0.2694 | AUC=0.9768/0.9459
2025-12-02 10:30:35 - [Layer 1] Ep 300: Loss=0.4125/0.2731 | AUC=0.9808/0.9454
2025-12-02 10:30:37 - [Layer 1] Ep 350: Loss=0.4088/0.2750 | AUC=0.9817/0.9453
2025-12-02 10:30:39 - [Layer 1] Ep 400: Loss=0.4031/0.2769 | AUC=0.9834/0.9452
2025-12-02 10:30:41 - [Layer 1] Ep 450: Loss=0.3892/0.2822 | AUC=0.9855/0.9439
2025-12-02 10:30:43 - [Layer 1] Ep 500: Loss=0.3844/0.2829 | AUC=0.9860/0.9437
2025-12-02 10:30:45 - [Layer 1] Ep 550: Loss=0.3887/0.2845 | AUC=0.9850/0.9435
2025-12-02 10:30:46 - |This is the 2th layer!|
2025-12-02 10:30:47 - [Layer 2] Ep 0: Loss=1.0307/0.6775 | AUC=0.9205/0.9077
2025-12-02 10:30:49 - [Layer 2] Ep 50: Loss=0.9367/0.6399 | AUC=0.8789/0.8870
2025-12-02 10:30:51 - [Layer 2] Ep 100: Loss=0.8783/0.6074 | AUC=0.8817/0.8888
2025-12-02 10:30:53 - [Layer 2] Ep 150: Loss=0.8518/0.5849 | AUC=0.8836/0.8897
2025-12-02 10:30:55 - [Layer 2] Ep 200: Loss=0.8399/0.5712 | AUC=0.8844/0.8903
2025-12-02 10:30:56 - [Layer 2] Ep 250: Loss=0.8261/0.5642 | AUC=0.8845/0.8911
2025-12-02 10:30:58 - [Layer 2] Ep 300: Loss=0.8217/0.5610 | AUC=0.8851/0.8916
2025-12-02 10:31:00 - [Layer 2] Ep 350: Loss=0.8193/0.5588 | AUC=0.8869/0.8917
2025-12-02 10:31:02 - [Layer 2] Ep 400: Loss=0.8148/0.5579 | AUC=0.8862/0.8920
2025-12-02 10:31:04 - [Layer 2] Ep 450: Loss=0.8170/0.5577 | AUC=0.8866/0.8924
2025-12-02 10:31:06 - [Layer 2] Ep 500: Loss=0.8151/0.5569 | AUC=0.8865/0.8926
2025-12-02 10:31:07 - [Layer 2] Ep 550: Loss=0.8178/0.5566 | AUC=0.8863/0.8926
2025-12-02 10:31:09 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9418    0.9543    0.9480       831
           1     0.9000    0.8747    0.8872       391

    accuracy                         0.9288      1222
   macro avg     0.9209    0.9145    0.9176      1222
weighted avg     0.9284    0.9288    0.9285      1222

2025-12-02 10:31:09 - Final Result -> Macro AUC: 0.9486, Macro F1: 0.9176, Macro Recall: 0.9145, G-mean: 0.9136, Acc: 0.9288
2025-12-02 10:32:21 - 
==================== New Experiment Started: 2025-12-02_10-32-21 ====================
2025-12-02 10:32:21 - Run Directory: training_records_sichuan_3/2025-12-02_10-32-21
2025-12-02 10:32:21 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.002, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=800, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:32:21 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:32:22 - |This is the 1th layer!|
2025-12-02 10:32:25 - [Layer 1] Ep 0: Loss=1.3680/0.6743 | AUC=0.5305/0.6998
2025-12-02 10:32:27 - [Layer 1] Ep 50: Loss=0.5385/0.2953 | AUC=0.9481/0.9347
2025-12-02 10:32:29 - [Layer 1] Ep 100: Loss=0.4814/0.2706 | AUC=0.9652/0.9445
2025-12-02 10:32:31 - [Layer 1] Ep 150: Loss=0.4609/0.2662 | AUC=0.9693/0.9458
2025-12-02 10:32:33 - [Layer 1] Ep 200: Loss=0.4425/0.2679 | AUC=0.9749/0.9463
2025-12-02 10:32:34 - [Layer 1] Ep 250: Loss=0.4217/0.2694 | AUC=0.9768/0.9459
2025-12-02 10:32:36 - [Layer 1] Ep 300: Loss=0.4127/0.2731 | AUC=0.9808/0.9454
2025-12-02 10:32:38 - [Layer 1] Ep 350: Loss=0.4083/0.2750 | AUC=0.9817/0.9453
2025-12-02 10:32:40 - [Layer 1] Ep 400: Loss=0.4037/0.2769 | AUC=0.9834/0.9452
2025-12-02 10:32:42 - [Layer 1] Ep 450: Loss=0.3891/0.2822 | AUC=0.9855/0.9439
2025-12-02 10:32:44 - [Layer 1] Ep 500: Loss=0.3845/0.2829 | AUC=0.9860/0.9437
2025-12-02 10:32:46 - [Layer 1] Ep 550: Loss=0.3885/0.2845 | AUC=0.9850/0.9435
2025-12-02 10:32:48 - [Layer 1] Ep 600: Loss=0.3806/0.2898 | AUC=0.9876/0.9437
2025-12-02 10:32:50 - [Layer 1] Ep 650: Loss=0.3761/0.2949 | AUC=0.9882/0.9431
2025-12-02 10:32:52 - [Layer 1] Ep 700: Loss=0.3733/0.2940 | AUC=0.9890/0.9438
2025-12-02 10:32:54 - [Layer 1] Ep 750: Loss=0.3659/0.2993 | AUC=0.9898/0.9431
2025-12-02 10:32:55 - |This is the 2th layer!|
2025-12-02 10:32:56 - [Layer 2] Ep 0: Loss=1.0299/0.6765 | AUC=0.9244/0.9089
2025-12-02 10:32:57 - [Layer 2] Ep 50: Loss=0.9292/0.6351 | AUC=0.8788/0.8861
2025-12-02 10:32:59 - [Layer 2] Ep 100: Loss=0.8698/0.6024 | AUC=0.8802/0.8876
2025-12-02 10:33:01 - [Layer 2] Ep 150: Loss=0.8438/0.5784 | AUC=0.8848/0.8897
2025-12-02 10:33:03 - [Layer 2] Ep 200: Loss=0.8263/0.5644 | AUC=0.8850/0.8903
2025-12-02 10:33:05 - [Layer 2] Ep 250: Loss=0.8213/0.5571 | AUC=0.8864/0.8913
2025-12-02 10:33:07 - [Layer 2] Ep 300: Loss=0.8161/0.5535 | AUC=0.8870/0.8920
2025-12-02 10:33:09 - [Layer 2] Ep 350: Loss=0.8168/0.5516 | AUC=0.8874/0.8923
2025-12-02 10:33:11 - [Layer 2] Ep 400: Loss=0.8103/0.5509 | AUC=0.8872/0.8925
2025-12-02 10:33:12 - [Layer 2] Ep 450: Loss=0.8113/0.5504 | AUC=0.8880/0.8926
2025-12-02 10:33:14 - [Layer 2] Ep 500: Loss=0.8093/0.5500 | AUC=0.8875/0.8927
2025-12-02 10:33:16 - [Layer 2] Ep 550: Loss=0.8100/0.5494 | AUC=0.8877/0.8924
2025-12-02 10:33:18 - [Layer 2] Ep 600: Loss=0.8083/0.5495 | AUC=0.8884/0.8928
2025-12-02 10:33:20 - [Layer 2] Ep 650: Loss=0.8090/0.5493 | AUC=0.8888/0.8929
2025-12-02 10:33:22 - [Layer 2] Ep 700: Loss=0.8084/0.5492 | AUC=0.8877/0.8928
2025-12-02 10:33:24 - [Layer 2] Ep 750: Loss=0.8127/0.5489 | AUC=0.8879/0.8926
2025-12-02 10:33:25 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9418    0.9543    0.9480       831
           1     0.9000    0.8747    0.8872       391

    accuracy                         0.9288      1222
   macro avg     0.9209    0.9145    0.9176      1222
weighted avg     0.9284    0.9288    0.9285      1222

2025-12-02 10:33:25 - Final Result -> Macro AUC: 0.9495, Macro F1: 0.9176, Macro Recall: 0.9145, G-mean: 0.9136, Acc: 0.9288
2025-12-02 10:35:08 - 
==================== New Experiment Started: 2025-12-02_10-35-08 ====================
2025-12-02 10:35:08 - Run Directory: training_records_sichuan_3/2025-12-02_10-35-08
2025-12-02 10:35:08 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.002, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=1000, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:35:08 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:35:08 - |This is the 1th layer!|
2025-12-02 10:35:10 - [Layer 1] Ep 0: Loss=1.3680/0.6743 | AUC=0.5305/0.6998
2025-12-02 10:35:12 - [Layer 1] Ep 50: Loss=0.5385/0.2953 | AUC=0.9481/0.9347
2025-12-02 10:35:14 - [Layer 1] Ep 100: Loss=0.4814/0.2706 | AUC=0.9652/0.9445
2025-12-02 10:35:16 - [Layer 1] Ep 150: Loss=0.4609/0.2662 | AUC=0.9693/0.9458
2025-12-02 10:35:18 - [Layer 1] Ep 200: Loss=0.4425/0.2679 | AUC=0.9749/0.9463
2025-12-02 10:35:20 - [Layer 1] Ep 250: Loss=0.4217/0.2694 | AUC=0.9768/0.9459
2025-12-02 10:35:21 - [Layer 1] Ep 300: Loss=0.4129/0.2731 | AUC=0.9808/0.9454
2025-12-02 10:35:24 - [Layer 1] Ep 350: Loss=0.4083/0.2750 | AUC=0.9817/0.9453
2025-12-02 10:35:25 - [Layer 1] Ep 400: Loss=0.4035/0.2769 | AUC=0.9834/0.9452
2025-12-02 10:35:27 - [Layer 1] Ep 450: Loss=0.3893/0.2822 | AUC=0.9855/0.9439
2025-12-02 10:35:29 - [Layer 1] Ep 500: Loss=0.3847/0.2829 | AUC=0.9860/0.9437
2025-12-02 10:35:31 - [Layer 1] Ep 550: Loss=0.3880/0.2845 | AUC=0.9850/0.9435
2025-12-02 10:35:33 - [Layer 1] Ep 600: Loss=0.3805/0.2898 | AUC=0.9876/0.9437
2025-12-02 10:35:35 - [Layer 1] Ep 650: Loss=0.3760/0.2949 | AUC=0.9882/0.9431
2025-12-02 10:35:37 - [Layer 1] Ep 700: Loss=0.3739/0.2940 | AUC=0.9890/0.9438
2025-12-02 10:35:39 - [Layer 1] Ep 750: Loss=0.3661/0.2993 | AUC=0.9898/0.9431
2025-12-02 10:35:41 - [Layer 1] Ep 800: Loss=0.3726/0.3029 | AUC=0.9901/0.9425
2025-12-02 10:35:43 - [Layer 1] Ep 850: Loss=0.3676/0.3022 | AUC=0.9884/0.9427
2025-12-02 10:35:44 - [Layer 1] Ep 900: Loss=0.3654/0.3060 | AUC=0.9909/0.9412
2025-12-02 10:35:47 - [Layer 1] Ep 950: Loss=0.3645/0.3098 | AUC=0.9913/0.9408
2025-12-02 10:35:48 - |This is the 2th layer!|
2025-12-02 10:35:49 - [Layer 2] Ep 0: Loss=1.0280/0.6754 | AUC=0.9254/0.9082
2025-12-02 10:35:50 - [Layer 2] Ep 50: Loss=0.9253/0.6317 | AUC=0.8813/0.8867
2025-12-02 10:35:52 - [Layer 2] Ep 100: Loss=0.8673/0.5992 | AUC=0.8816/0.8885
2025-12-02 10:35:54 - [Layer 2] Ep 150: Loss=0.8446/0.5762 | AUC=0.8844/0.8901
2025-12-02 10:35:56 - [Layer 2] Ep 200: Loss=0.8249/0.5615 | AUC=0.8860/0.8912
2025-12-02 10:35:58 - [Layer 2] Ep 250: Loss=0.8191/0.5542 | AUC=0.8876/0.8920
2025-12-02 10:36:00 - [Layer 2] Ep 300: Loss=0.8139/0.5506 | AUC=0.8877/0.8923
2025-12-02 10:36:02 - [Layer 2] Ep 350: Loss=0.8123/0.5487 | AUC=0.8880/0.8923
2025-12-02 10:36:03 - [Layer 2] Ep 400: Loss=0.8092/0.5481 | AUC=0.8890/0.8925
2025-12-02 10:36:05 - [Layer 2] Ep 450: Loss=0.8095/0.5476 | AUC=0.8895/0.8927
2025-12-02 10:36:07 - [Layer 2] Ep 500: Loss=0.8087/0.5472 | AUC=0.8880/0.8927
2025-12-02 10:36:09 - [Layer 2] Ep 550: Loss=0.8126/0.5468 | AUC=0.8887/0.8926
2025-12-02 10:36:11 - [Layer 2] Ep 600: Loss=0.8108/0.5467 | AUC=0.8893/0.8929
2025-12-02 10:36:13 - [Layer 2] Ep 650: Loss=0.8070/0.5467 | AUC=0.8891/0.8930
2025-12-02 10:36:15 - [Layer 2] Ep 700: Loss=0.8103/0.5464 | AUC=0.8889/0.8928
2025-12-02 10:36:16 - [Layer 2] Ep 750: Loss=0.8075/0.5465 | AUC=0.8895/0.8928
2025-12-02 10:36:18 - [Layer 2] Ep 800: Loss=0.8056/0.5464 | AUC=0.8884/0.8929
2025-12-02 10:36:20 - [Layer 2] Ep 850: Loss=0.8109/0.5464 | AUC=0.8900/0.8928
2025-12-02 10:36:22 - [Layer 2] Ep 900: Loss=0.8054/0.5464 | AUC=0.8892/0.8929
2025-12-02 10:36:24 - [Layer 2] Ep 950: Loss=0.8081/0.5465 | AUC=0.8884/0.8929
2025-12-02 10:36:25 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9375    0.9567    0.9470       831
           1     0.9037    0.8645    0.8837       391

    accuracy                         0.9272      1222
   macro avg     0.9206    0.9106    0.9153      1222
weighted avg     0.9267    0.9272    0.9267      1222

2025-12-02 10:36:25 - Final Result -> Macro AUC: 0.9482, Macro F1: 0.9153, Macro Recall: 0.9106, G-mean: 0.9094, Acc: 0.9272
2025-12-02 10:40:51 - 
==================== New Experiment Started: 2025-12-02_10-40-51 ====================
2025-12-02 10:40:51 - Run Directory: training_records_sichuan_3/2025-12-02_10-40-51
2025-12-02 10:40:51 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.001, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=1000, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:40:51 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:40:51 - |This is the 1th layer!|
2025-12-02 10:40:53 - [Layer 1] Ep 0: Loss=1.3680/0.6816 | AUC=0.5305/0.6046
2025-12-02 10:40:55 - [Layer 1] Ep 50: Loss=0.6083/0.3395 | AUC=0.9312/0.9258
2025-12-02 10:40:57 - [Layer 1] Ep 100: Loss=0.5328/0.2885 | AUC=0.9538/0.9382
2025-12-02 10:40:59 - [Layer 1] Ep 150: Loss=0.5033/0.2730 | AUC=0.9602/0.9433
2025-12-02 10:41:01 - [Layer 1] Ep 200: Loss=0.4840/0.2674 | AUC=0.9659/0.9453
2025-12-02 10:41:03 - [Layer 1] Ep 250: Loss=0.4596/0.2655 | AUC=0.9687/0.9458
2025-12-02 10:41:05 - [Layer 1] Ep 300: Loss=0.4502/0.2681 | AUC=0.9723/0.9456
2025-12-02 10:41:07 - [Layer 1] Ep 350: Loss=0.4476/0.2660 | AUC=0.9740/0.9459
2025-12-02 10:41:09 - [Layer 1] Ep 400: Loss=0.4384/0.2672 | AUC=0.9758/0.9457
2025-12-02 10:41:11 - [Layer 1] Ep 450: Loss=0.4216/0.2679 | AUC=0.9790/0.9452
2025-12-02 10:41:13 - [Layer 1] Ep 500: Loss=0.4178/0.2691 | AUC=0.9788/0.9454
2025-12-02 10:41:15 - [Layer 1] Ep 550: Loss=0.4195/0.2691 | AUC=0.9786/0.9452
2025-12-02 10:41:17 - [Layer 1] Ep 600: Loss=0.4104/0.2737 | AUC=0.9812/0.9446
2025-12-02 10:41:19 - [Layer 1] Ep 650: Loss=0.4046/0.2758 | AUC=0.9822/0.9441
2025-12-02 10:41:21 - [Layer 1] Ep 700: Loss=0.4012/0.2755 | AUC=0.9830/0.9446
2025-12-02 10:41:23 - [Layer 1] Ep 750: Loss=0.3924/0.2780 | AUC=0.9839/0.9443
2025-12-02 10:41:25 - [Layer 1] Ep 800: Loss=0.3982/0.2808 | AUC=0.9849/0.9431
2025-12-02 10:41:27 - [Layer 1] Ep 850: Loss=0.3953/0.2828 | AUC=0.9823/0.9439
2025-12-02 10:41:29 - [Layer 1] Ep 900: Loss=0.3859/0.2847 | AUC=0.9863/0.9431
2025-12-02 10:41:31 - [Layer 1] Ep 950: Loss=0.3889/0.2870 | AUC=0.9877/0.9423
2025-12-02 10:41:32 - |This is the 2th layer!|
2025-12-02 10:41:33 - [Layer 2] Ep 0: Loss=1.0327/0.6787 | AUC=0.9221/0.9086
2025-12-02 10:41:35 - [Layer 2] Ep 50: Loss=0.9848/0.6610 | AUC=0.8879/0.8922
2025-12-02 10:41:37 - [Layer 2] Ep 100: Loss=0.9392/0.6420 | AUC=0.8820/0.8891
2025-12-02 10:41:39 - [Layer 2] Ep 150: Loss=0.9066/0.6239 | AUC=0.8813/0.8882
2025-12-02 10:41:41 - [Layer 2] Ep 200: Loss=0.8792/0.6081 | AUC=0.8819/0.8885
2025-12-02 10:41:43 - [Layer 2] Ep 250: Loss=0.8641/0.5952 | AUC=0.8828/0.8890
2025-12-02 10:41:45 - [Layer 2] Ep 300: Loss=0.8524/0.5848 | AUC=0.8838/0.8892
2025-12-02 10:41:47 - [Layer 2] Ep 350: Loss=0.8443/0.5769 | AUC=0.8839/0.8896
2025-12-02 10:41:48 - [Layer 2] Ep 400: Loss=0.8369/0.5711 | AUC=0.8852/0.8900
2025-12-02 10:41:50 - [Layer 2] Ep 450: Loss=0.8331/0.5666 | AUC=0.8860/0.8903
2025-12-02 10:41:52 - [Layer 2] Ep 500: Loss=0.8286/0.5634 | AUC=0.8854/0.8906
2025-12-02 10:41:54 - [Layer 2] Ep 550: Loss=0.8300/0.5609 | AUC=0.8864/0.8906
2025-12-02 10:41:56 - [Layer 2] Ep 600: Loss=0.8260/0.5592 | AUC=0.8873/0.8912
2025-12-02 10:41:58 - [Layer 2] Ep 650: Loss=0.8211/0.5581 | AUC=0.8873/0.8914
2025-12-02 10:42:00 - [Layer 2] Ep 700: Loss=0.8234/0.5571 | AUC=0.8875/0.8914
2025-12-02 10:42:02 - [Layer 2] Ep 750: Loss=0.8196/0.5564 | AUC=0.8881/0.8915
2025-12-02 10:42:04 - [Layer 2] Ep 800: Loss=0.8166/0.5558 | AUC=0.8870/0.8915
2025-12-02 10:42:06 - [Layer 2] Ep 850: Loss=0.8208/0.5553 | AUC=0.8886/0.8916
2025-12-02 10:42:08 - [Layer 2] Ep 900: Loss=0.8144/0.5550 | AUC=0.8881/0.8918
2025-12-02 10:42:10 - [Layer 2] Ep 950: Loss=0.8164/0.5549 | AUC=0.8876/0.8918
2025-12-02 10:42:11 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9419    0.9555    0.9486       831
           1     0.9024    0.8747    0.8883       391

    accuracy                         0.9296      1222
   macro avg     0.9221    0.9151    0.9185      1222
weighted avg     0.9292    0.9296    0.9293      1222

2025-12-02 10:42:11 - Final Result -> Macro AUC: 0.9506, Macro F1: 0.9185, Macro Recall: 0.9151, G-mean: 0.9142, Acc: 0.9296
2025-12-02 10:44:17 - 
==================== New Experiment Started: 2025-12-02_10-44-17 ====================
2025-12-02 10:44:17 - Run Directory: training_records_sichuan_3/2025-12-02_10-44-17
2025-12-02 10:44:17 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.0001, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=1000, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:44:18 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:44:18 - |This is the 1th layer!|
2025-12-02 10:44:20 - [Layer 1] Ep 0: Loss=1.3680/0.6886 | AUC=0.5305/0.5149
2025-12-02 10:44:22 - [Layer 1] Ep 50: Loss=1.0508/0.6383 | AUC=0.8172/0.8563
2025-12-02 10:44:23 - [Layer 1] Ep 100: Loss=0.9081/0.5733 | AUC=0.8797/0.8856
2025-12-02 10:44:25 - [Layer 1] Ep 150: Loss=0.8641/0.5149 | AUC=0.8962/0.8977
2025-12-02 10:44:27 - [Layer 1] Ep 200: Loss=0.7685/0.4691 | AUC=0.9115/0.9071
2025-12-02 10:44:29 - [Layer 1] Ep 250: Loss=0.7381/0.4328 | AUC=0.9186/0.9144
2025-12-02 10:44:31 - [Layer 1] Ep 300: Loss=0.6933/0.4040 | AUC=0.9231/0.9199
2025-12-02 10:44:33 - [Layer 1] Ep 350: Loss=0.6709/0.3808 | AUC=0.9307/0.9234
2025-12-02 10:44:35 - [Layer 1] Ep 400: Loss=0.6516/0.3626 | AUC=0.9314/0.9264
2025-12-02 10:44:36 - [Layer 1] Ep 450: Loss=0.6218/0.3476 | AUC=0.9376/0.9287
2025-12-02 10:44:38 - [Layer 1] Ep 500: Loss=0.6040/0.3362 | AUC=0.9378/0.9303
2025-12-02 10:44:40 - [Layer 1] Ep 550: Loss=0.5959/0.3260 | AUC=0.9425/0.9317
2025-12-02 10:44:42 - [Layer 1] Ep 600: Loss=0.5873/0.3174 | AUC=0.9443/0.9333
2025-12-02 10:44:44 - [Layer 1] Ep 650: Loss=0.5781/0.3106 | AUC=0.9453/0.9347
2025-12-02 10:44:46 - [Layer 1] Ep 700: Loss=0.5655/0.3049 | AUC=0.9481/0.9359
2025-12-02 10:44:48 - [Layer 1] Ep 750: Loss=0.5583/0.2994 | AUC=0.9499/0.9369
2025-12-02 10:44:50 - [Layer 1] Ep 800: Loss=0.5586/0.2955 | AUC=0.9507/0.9378
2025-12-02 10:44:52 - [Layer 1] Ep 850: Loss=0.5493/0.2914 | AUC=0.9512/0.9386
2025-12-02 10:44:54 - [Layer 1] Ep 900: Loss=0.5391/0.2882 | AUC=0.9520/0.9393
2025-12-02 10:44:56 - [Layer 1] Ep 950: Loss=0.6391/0.2857 | AUC=0.9566/0.9400
2025-12-02 10:44:57 - |This is the 2th layer!|
2025-12-02 10:44:58 - [Layer 2] Ep 0: Loss=1.0310/0.6857 | AUC=0.9012/0.9045
2025-12-02 10:45:00 - [Layer 2] Ep 50: Loss=1.0290/0.6853 | AUC=0.8975/0.9032
2025-12-02 10:45:02 - [Layer 2] Ep 100: Loss=1.0265/0.6848 | AUC=0.8928/0.9005
2025-12-02 10:45:04 - [Layer 2] Ep 150: Loss=1.0248/0.6843 | AUC=0.8902/0.8975
2025-12-02 10:45:06 - [Layer 2] Ep 200: Loss=1.0221/0.6838 | AUC=0.8877/0.8946
2025-12-02 10:45:07 - [Layer 2] Ep 250: Loss=1.0198/0.6832 | AUC=0.8860/0.8922
2025-12-02 10:45:09 - [Layer 2] Ep 300: Loss=1.0170/0.6826 | AUC=0.8835/0.8903
2025-12-02 10:45:11 - [Layer 2] Ep 350: Loss=1.0151/0.6820 | AUC=0.8821/0.8884
2025-12-02 10:45:13 - [Layer 2] Ep 400: Loss=1.0130/0.6813 | AUC=0.8810/0.8872
2025-12-02 10:45:15 - [Layer 2] Ep 450: Loss=1.0099/0.6805 | AUC=0.8794/0.8859
2025-12-02 10:45:17 - [Layer 2] Ep 500: Loss=1.0072/0.6796 | AUC=0.8783/0.8851
2025-12-02 10:45:19 - [Layer 2] Ep 550: Loss=1.0051/0.6788 | AUC=0.8794/0.8845
2025-12-02 10:45:21 - [Layer 2] Ep 600: Loss=1.0016/0.6778 | AUC=0.8791/0.8840
2025-12-02 10:45:22 - [Layer 2] Ep 650: Loss=0.9982/0.6768 | AUC=0.8770/0.8835
2025-12-02 10:45:24 - [Layer 2] Ep 700: Loss=0.9968/0.6758 | AUC=0.8775/0.8832
2025-12-02 10:45:26 - [Layer 2] Ep 750: Loss=0.9929/0.6747 | AUC=0.8765/0.8828
2025-12-02 10:45:28 - [Layer 2] Ep 800: Loss=0.9899/0.6735 | AUC=0.8762/0.8826
2025-12-02 10:45:30 - [Layer 2] Ep 850: Loss=0.9872/0.6723 | AUC=0.8781/0.8823
2025-12-02 10:45:32 - [Layer 2] Ep 900: Loss=0.9822/0.6711 | AUC=0.8775/0.8820
2025-12-02 10:45:34 - [Layer 2] Ep 950: Loss=0.9800/0.6698 | AUC=0.8761/0.8818
2025-12-02 10:45:35 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9240    0.9362    0.9301       831
           1     0.8605    0.8363    0.8482       391

    accuracy                         0.9043      1222
   macro avg     0.8923    0.8863    0.8892      1222
weighted avg     0.9037    0.9043    0.9039      1222

2025-12-02 10:45:35 - Final Result -> Macro AUC: 0.9466, Macro F1: 0.8892, Macro Recall: 0.8863, G-mean: 0.8849, Acc: 0.9043
2025-12-02 10:46:27 - 
==================== New Experiment Started: 2025-12-02_10-46-27 ====================
2025-12-02 10:46:27 - Run Directory: training_records_sichuan_3/2025-12-02_10-46-27
2025-12-02 10:46:27 - Arguments: Namespace(dataset='Sichuan', train_size=0.6, lr=0.001, hid=128, dropout=0, adj_dropout=0.4, attn_drop=0.5, in_drop=0.1, attention_weight=0.1, feature_weight=0.1, layers=2, num_layers=1, num_heads=1, num_out_heads=1, weight_decay=0.001, epochs=1000, patience=60, early_stop=False, residual=False, negative_slope=0.2, att_loss_weight=0.5, print_interval=50, IR=0.1, IR_set=0, cost=2, seed=42, blank=0, lambda_I=1.0, lambda_G=1.0, lambda_D=1.0)
2025-12-02 10:46:27 - Initializing weights using Static HAS-GNN Priors...
2025-12-02 10:46:27 - |This is the 1th layer!|
2025-12-02 10:46:31 - [Layer 1] Ep 0: Loss=1.3680/0.6816 | AUC=0.5305/0.6046
2025-12-02 10:46:33 - [Layer 1] Ep 50: Loss=0.6083/0.3395 | AUC=0.9312/0.9258
2025-12-02 10:46:34 - [Layer 1] Ep 100: Loss=0.5328/0.2885 | AUC=0.9538/0.9382
2025-12-02 10:46:36 - [Layer 1] Ep 150: Loss=0.5033/0.2730 | AUC=0.9602/0.9433
2025-12-02 10:46:38 - [Layer 1] Ep 200: Loss=0.4840/0.2674 | AUC=0.9659/0.9453
2025-12-02 10:46:40 - [Layer 1] Ep 250: Loss=0.4596/0.2655 | AUC=0.9687/0.9458
2025-12-02 10:46:42 - [Layer 1] Ep 300: Loss=0.4502/0.2681 | AUC=0.9723/0.9456
2025-12-02 10:46:44 - [Layer 1] Ep 350: Loss=0.4477/0.2660 | AUC=0.9740/0.9459
2025-12-02 10:46:46 - [Layer 1] Ep 400: Loss=0.4384/0.2672 | AUC=0.9758/0.9457
2025-12-02 10:46:48 - [Layer 1] Ep 450: Loss=0.4217/0.2679 | AUC=0.9790/0.9452
2025-12-02 10:46:50 - [Layer 1] Ep 500: Loss=0.4178/0.2691 | AUC=0.9788/0.9454
2025-12-02 10:46:52 - [Layer 1] Ep 550: Loss=0.4195/0.2691 | AUC=0.9786/0.9452
2025-12-02 10:46:54 - [Layer 1] Ep 600: Loss=0.4104/0.2737 | AUC=0.9812/0.9446
2025-12-02 10:46:56 - [Layer 1] Ep 650: Loss=0.4046/0.2758 | AUC=0.9822/0.9441
2025-12-02 10:46:58 - [Layer 1] Ep 700: Loss=0.4010/0.2755 | AUC=0.9830/0.9446
2025-12-02 10:47:00 - [Layer 1] Ep 750: Loss=0.3925/0.2780 | AUC=0.9839/0.9443
2025-12-02 10:47:01 - [Layer 1] Ep 800: Loss=0.3982/0.2808 | AUC=0.9849/0.9431
2025-12-02 10:47:03 - [Layer 1] Ep 850: Loss=0.3953/0.2828 | AUC=0.9823/0.9439
2025-12-02 10:47:05 - [Layer 1] Ep 900: Loss=0.3859/0.2847 | AUC=0.9863/0.9431
2025-12-02 10:47:07 - [Layer 1] Ep 950: Loss=0.3887/0.2870 | AUC=0.9877/0.9423
2025-12-02 10:47:09 - |This is the 2th layer!|
2025-12-02 10:47:09 - [Layer 2] Ep 0: Loss=1.0327/0.6787 | AUC=0.9221/0.9086
2025-12-02 10:47:11 - [Layer 2] Ep 50: Loss=0.9848/0.6609 | AUC=0.8879/0.8922
2025-12-02 10:47:13 - [Layer 2] Ep 100: Loss=0.9391/0.6420 | AUC=0.8819/0.8891
2025-12-02 10:47:15 - [Layer 2] Ep 150: Loss=0.9065/0.6239 | AUC=0.8812/0.8881
2025-12-02 10:47:17 - [Layer 2] Ep 200: Loss=0.8793/0.6081 | AUC=0.8817/0.8884
2025-12-02 10:47:19 - [Layer 2] Ep 250: Loss=0.8642/0.5952 | AUC=0.8826/0.8889
2025-12-02 10:47:21 - [Layer 2] Ep 300: Loss=0.8525/0.5849 | AUC=0.8837/0.8892
2025-12-02 10:47:22 - [Layer 2] Ep 350: Loss=0.8443/0.5769 | AUC=0.8839/0.8895
2025-12-02 10:47:24 - [Layer 2] Ep 400: Loss=0.8369/0.5710 | AUC=0.8851/0.8898
2025-12-02 10:47:26 - [Layer 2] Ep 450: Loss=0.8331/0.5666 | AUC=0.8860/0.8902
2025-12-02 10:47:28 - [Layer 2] Ep 500: Loss=0.8286/0.5633 | AUC=0.8853/0.8905
2025-12-02 10:47:30 - [Layer 2] Ep 550: Loss=0.8299/0.5609 | AUC=0.8863/0.8906
2025-12-02 10:47:32 - [Layer 2] Ep 600: Loss=0.8260/0.5592 | AUC=0.8872/0.8912
2025-12-02 10:47:34 - [Layer 2] Ep 650: Loss=0.8211/0.5581 | AUC=0.8873/0.8914
2025-12-02 10:47:36 - [Layer 2] Ep 700: Loss=0.8233/0.5570 | AUC=0.8875/0.8914
2025-12-02 10:47:37 - [Layer 2] Ep 750: Loss=0.8196/0.5563 | AUC=0.8880/0.8915
2025-12-02 10:47:39 - [Layer 2] Ep 800: Loss=0.8165/0.5558 | AUC=0.8870/0.8915
2025-12-02 10:47:41 - [Layer 2] Ep 850: Loss=0.8208/0.5553 | AUC=0.8885/0.8917
2025-12-02 10:47:43 - [Layer 2] Ep 900: Loss=0.8143/0.5550 | AUC=0.8882/0.8918
2025-12-02 10:47:45 - [Layer 2] Ep 950: Loss=0.8164/0.5549 | AUC=0.8876/0.8919
2025-12-02 10:47:46 - 
Final Test Report:
              precision    recall  f1-score   support

           0     0.9419    0.9555    0.9486       831
           1     0.9024    0.8747    0.8883       391

    accuracy                         0.9296      1222
   macro avg     0.9221    0.9151    0.9185      1222
weighted avg     0.9292    0.9296    0.9293      1222

2025-12-02 10:47:46 - Final Result -> Macro AUC: 0.9506, Macro F1: 0.9185, Macro Recall: 0.9151, G-mean: 0.9142, Acc: 0.9296
